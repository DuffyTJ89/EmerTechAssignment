{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center><font color='Red' text align='center'>Digit recognition script</font></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will cover how the script is used and what I have done to try to get the accuracy as high as I can.\n",
    "\n",
    "This is the main code from the script file and it is currently working at a 97% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.5692 - acc: 0.8339\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 11s 179us/step - loss: 0.2518 - acc: 0.9229\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.1801 - acc: 0.9447\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.1595 - acc: 0.9501\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 11s 178us/step - loss: 0.1454 - acc: 0.9553\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.1377 - acc: 0.9570\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.1214 - acc: 0.9624\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.1136 - acc: 0.9649\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.1131 - acc: 0.9656\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.1065 - acc: 0.9671\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 11s 192us/step - loss: 0.1028 - acc: 0.9684\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 11s 191us/step - loss: 0.1044 - acc: 0.9685\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0981 - acc: 0.9703\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0876 - acc: 0.9731\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.0957 - acc: 0.9714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d9e41e5f8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('data/t10k-images-idx3-ubyte.gz', 'rb') as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "# Import keras.\n",
    "import keras as kr\n",
    "\n",
    "# Start a neural network, building it by layers.\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with 1000 neurons and an input layer with 784.\n",
    "model.add(kr.layers.Dense(units=600, activation='linear', input_dim=784))\n",
    "model.add(kr.layers.Dense(units=400, activation='relu'))\n",
    "# Add a three neuron output layer.\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Build the graph.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "\n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()\n",
    "    \n",
    "import numpy as np\n",
    "        \n",
    "train_img = ~np.array(list(train_img[16:])).reshape(60000, 28, 28).astype(np.uint8) / 255.0\n",
    "train_lbl =  np.array(list(train_lbl[ 8:])).astype(np.uint8)\n",
    "\n",
    "inputs = train_img.reshape(60000, 784)\n",
    "\n",
    " # For encoding categorical variables.\n",
    "import sklearn.preprocessing as pre\n",
    "\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(train_lbl)\n",
    "outputs = encoder.transform(train_lbl)\n",
    "\n",
    "model.fit(inputs, outputs, epochs=15, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trying to improve accuracy\n",
    "\n",
    "There are a few condations we can change to try to improve accuracy. \n",
    "\n",
    "Looking at the following line :\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "we can see using https://keras.io/optimizers/ that there are a number of different optimizers available so how do we find the best one.\n",
    "\n",
    "if we change the optimizer to Nadam we can see the network performs worse, much worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 14.3056 - acc: 0.1000\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 14.5463 - acc: 0.0975\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 13s 211us/step - loss: 14.5463 - acc: 0.0975\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 14.5463 - acc: 0.0975\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 13s 209us/step - loss: 14.5463 - acc: 0.0975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23da46170b8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a neural network, building it by layers.\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with 1000 neurons and an input layer with 784.\n",
    "model.add(kr.layers.Dense(units=600, activation='linear', input_dim=784))\n",
    "model.add(kr.layers.Dense(units=400, activation='relu'))\n",
    "# Add a three neuron output layer.\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Build the graph.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])\n",
    "\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "\n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()\n",
    "    \n",
    "import numpy as np\n",
    "        \n",
    "train_img = ~np.array(list(train_img[16:])).reshape(60000, 28, 28).astype(np.uint8) / 255.0\n",
    "train_lbl =  np.array(list(train_lbl[ 8:])).astype(np.uint8)\n",
    "\n",
    "inputs = train_img.reshape(60000, 784)\n",
    "\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(train_lbl)\n",
    "outputs = encoder.transform(train_lbl)\n",
    "\n",
    "model.fit(inputs, outputs, epochs=5, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network is actually most effective on it's first run and then remains as a constant level after. It's learning nothing new from each run and this is because of the optimizer Nadam. It is obviously not fit for purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Continuing to read through the optimizers documentation we can see adamax. \n",
    "\n",
    "If we change the optimizer to Adamax we can see the network performs worse than adam at the start but improves much better over each Epoch/run. The learning rate is better and running both Adam and adamax for 15 Epochs sees it overtake adam optimizer for accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 2.3980 - acc: 0.7290\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.7115 - acc: 0.8681\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.2584 - acc: 0.9210\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.1923 - acc: 0.9413\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.1572 - acc: 0.9510\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.1374 - acc: 0.9581\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.1183 - acc: 0.9628\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 0.1058 - acc: 0.9667\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0975 - acc: 0.9701\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.0896 - acc: 0.9710\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.0794 - acc: 0.9749\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.0746 - acc: 0.9760\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 11s 183us/step - loss: 0.0670 - acc: 0.9780\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 11s 181us/step - loss: 0.0639 - acc: 0.9800\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 11s 182us/step - loss: 0.0614 - acc: 0.9803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23da8ddc710>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a neural network, building it by layers.\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add a hidden layer with 1000 neurons and an input layer with 784.\n",
    "model.add(kr.layers.Dense(units=600, activation='linear', input_dim=784))\n",
    "model.add(kr.layers.Dense(units=400, activation='relu'))\n",
    "# Add a three neuron output layer.\n",
    "model.add(kr.layers.Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Build the graph.\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adamax', metrics=['accuracy'])\n",
    "\n",
    "with gzip.open('data/train-images-idx3-ubyte.gz', 'rb') as f:\n",
    "    train_img = f.read()\n",
    "\n",
    "with gzip.open('data/train-labels-idx1-ubyte.gz', 'rb') as f:\n",
    "    train_lbl = f.read()\n",
    "    \n",
    "import numpy as np\n",
    "        \n",
    "train_img = ~np.array(list(train_img[16:])).reshape(60000, 28, 28).astype(np.uint8) / 255.0\n",
    "train_lbl =  np.array(list(train_lbl[ 8:])).astype(np.uint8)\n",
    "\n",
    "inputs = train_img.reshape(60000, 784)\n",
    "\n",
    "encoder = pre.LabelBinarizer()\n",
    "encoder.fit(train_lbl)\n",
    "outputs = encoder.transform(train_lbl)\n",
    "\n",
    "model.fit(inputs, outputs, epochs=15, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So because of it's improved peformance Adamax is the better optmizer over the long run. Used in the script for 50 epochs each the Adam optimizer topped out around 98%, Adamax was able to get the accuracy to 99.5%. ON a 30 epoch run it will get an accuracy of 99%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://raw.githubusercontent.com/DuffyTJ89/EmerTechAssignment/master/images/DigitRec/ScriptOutcome.PNG?token=AVaH05ZmDNHEL4wOJ6Mevny5VBR-_JEnks5cCtMBwA%3D%3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
